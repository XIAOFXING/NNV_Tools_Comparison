<!DOCTYPE html>
<html>
<head>
<title>experiment_design.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="experimental-evaluation-of-neural-network-verification-tools">Experimental Evaluation of Neural Network Verification Tools</h1>
<h2 id="1-goal">1. Goal</h2>
<p>Assessing the Performance of State-of-the-Art Verifiers Across VNN COMP Iterations.</p>
<h2 id="2-problem-formalization">2. Problem formalization</h2>
<p><strong>Neural Network Verification Problem</strong>----We consider the neural network verification problem defined as follows:</p>
<p>Given an <strong>input specification</strong> $\phi \subseteq \mathbb{R}^{d_{\text{in}}}$ (also called <em>pre-condition</em>), an <strong>output specification</strong> $\psi \subseteq \mathbb{R}^{d_{\text{out}}}$ (also called <em>post-condition</em>), and a <strong>neural network</strong> $N : \mathbb{R}^{d_{\text{in}}} \mapsto \mathbb{R}^{d_{\text{out}}}$, we aim to prove that the pre-condition implies the post-condition:</p>
<p>$$
\forall x : x \vDash \phi \Rightarrow N(x) \vDash \psi
$$</p>
<p><strong>Adversarial Robustness in Image Classification</strong>: One particularly popular property is the robustness to adversarial $\ell_{\infty}$-norm bounded perturbations in image classification.</p>
<p>There, the network $N$ computes a numerical score $y \in \mathbb{R}^{d_{\text{out}}}$ corresponding to its confidence that the input belongs to each of the $d_{\text{out}}$ classes for each input $x \in \mathbb{R}^{d_{\text{in}}}$. The final classification $c$ is then computed as:</p>
<p>$$
c = \arg\max_{i} N(x)_i
$$</p>
<p>In this setting, an adversary may want to perturb the input such that the classification changes. Therefore, the verification intends to prove that:</p>
<p>$$
\arg\max_{i} N(x^{\prime})<em>i = t, \quad \forall x^{\prime} \in {x^{\prime} \in \mathbb{R}^{d</em>{\text{in}}} \mid |x - x^{\prime}|_{\infty} \leq \epsilon}
$$</p>
<p>where:</p>
<ul>
<li>$t$ is the target class</li>
<li>$x$ is the original image</li>
<li>$\epsilon$ is the maximal permissible perturbation magnitude</li>
</ul>
<p>Thus, the <strong>pre-condition $\phi$</strong> describes the inputs an attacker can choose from (an $\ell_{\infty}$-ball of radius $\epsilon$):</p>
<p>$$
\phi = {x^{\prime} \in \mathbb{R}^{d_{\text{in}}} \mid |x - x^{\prime}|_{\infty} \leq \epsilon}
$$</p>
<p>The <strong>post-condition $\psi$</strong> describes the output space corresponding to classification to the target class $t$:</p>
<p>$$
\psi = {y \in \mathbb{R}^{d_{\text{out}}} \mid y_t &gt; y_i, \ \forall i \neq t}
$$</p>
<h2 id="3-terminology">3. Terminology</h2>
<p><strong>Instance:</strong> An instance is defined by a property specification (pre- and post-condition), a network, and a timeout.
For example, one instance might consist of an MNIST classifier with one input image, a given local robustness threshold $\epsilon$, and a specific timeout.</p>
<p><strong>Benchmark:</strong> A benchmark is defined as a set of related instances. For example, one benchmark might consist of a specific MNIST classifier with 100 input images, potentially different robustness thresholds ε, and one timeout per input.</p>
<p><strong>Format:</strong> ONNX for neural networks and VNN-LIB for specifications.</p>
<h2 id="4-methods-and-algorithms-verifiers">4. Methods and Algorithms (Verifiers)</h2>
<p><strong>We have chosen the following verifiers for performance evaluation based on their strong overall results in previous VNN COMP and their open-source availability:</strong></p>
<table>
<thead>
<tr>
<th>Verifiers</th>
<th>2021</th>
<th>2022</th>
<th>2023</th>
<th>2024</th>
<th>2025</th>
<th>Hardware and Licenses</th>
<th>Participated Benchmarks</th>
<th>Link—Latest Version</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>α-β-CROWN</strong></td>
<td>1st</td>
<td>1st</td>
<td>1st</td>
<td>1st</td>
<td>1st</td>
<td>CPU and GPU with 32-bit or 64-bit floating point; Gurobi license required for certain benchmarks</td>
<td>All benchmarks</td>
<td><a href="https://github.com/Verified-Intelligence/alpha-beta-CROWN_vnncomp2025">GitHub</a></td>
<td>α,β-CROWN is an efficient neural network verifier based on the linear bound propagation framework and built on a series of works on boundpropagation-based neural network verifiers: CROWN, auto LiRPA, α-CROWN, β-CROWN, GCP-CROWN, GenBaB, BICCOS. The core techniques in α,βCROWN combine the efficient and GPU-accelerated linear bound propagation method with branch-and-bound methods specialized for neural network verification.</td>
</tr>
<tr>
<td><strong>nnenum</strong></td>
<td>8th</td>
<td>4th</td>
<td>5th</td>
<td>5th</td>
<td>5th</td>
<td>CPU, Gurobi license (optional)</td>
<td>acasxu, cgan, collins-rul-cnn, cora, linearizenn, metaroom, safeNLP, nn4sys, tlverifybench, vggnet16</td>
<td><a href="https://github.com/aliabigdeli/nnenum">GitHub</a></td>
<td>The nnenum tool uses multiple levels of abstraction to achieve high performance verification of ReLU networks without sacrificing completeness. The core verification method is based on reachability analysis using star sets, combined with the ImageStar method to propagate stes through all linear layers supported by the ONNX runtime, such as convolutional layers with arbitrary parameters.</td>
</tr>
<tr>
<td><strong>Marabou</strong></td>
<td>5th</td>
<td>7th</td>
<td>2nd</td>
<td>4th</td>
<td>4th</td>
<td>CPU, no license required. Can also be accelerated with Gurobi (which requires a license)</td>
<td>acasxu, cgan, collins rul cnn, list shift, linearizenn, metaroom, nn4sys, safenlp, verifybench, cifar100, tinyimagenet</td>
<td><a href="https://github.com/NeuralNetworkVerification/Marabou">GitHub</a></td>
<td>Marabou can answer queries about a network's properties by encoding and solving these queries as constraint satisfaction problems. Marabou supports many different linear, piecewise-linear, and non-linear operations and architectures (e.g., FFNNs, CNNs, residual connections, Graph Neural Networks). Marabou performs complete analysis that employs a specialized convex optimization procedure and abstract interpretation. It also uses the Split-and-Conquer algorithm for parallelization.</td>
</tr>
<tr>
<td><strong>NNV</strong></td>
<td>-</td>
<td>-</td>
<td>6th</td>
<td>6th</td>
<td>6th</td>
<td>CPU, MATLAB license</td>
<td>Regular track except for LinearizeNN</td>
<td><a href="https://github.com/verivital/nnv">GitHub</a></td>
<td>NNV uses a star-set state-space representation and reachability algorithm that allows for a layer-by-layer computation of exact or overapproximate reachable sets for feed-forward, convolutional, semantic segmentation (SSNN), and recurrent (RNN) neural networks, as well as neural network control systems (NNCS) and neural ordinary differential equations (Neural ODEs).</td>
</tr>
<tr>
<td><strong>NeuralSAT</strong></td>
<td>-</td>
<td>-</td>
<td>4th</td>
<td>2nd</td>
<td>2nd</td>
<td>GPU, Gurobi License</td>
<td>acasxu, cgan, collins-rul-cnn, dist-shift, nn4sys, vggnet16, tlverifybench, trafic-signs-recognition, reach-prob-density, metaroom</td>
<td><a href="https://github.com/dynaroars/neuralsat">GitHub</a></td>
<td>NeuralSAT integrates conflict-driven clause learning (CDCL) in SAT/SMT-solving with an DNN abstraction based theory solver for infeasibility checking. The design of NeuralSAT is inspired by the core algorithms used in SMT solvers such as CDCL components (light shades) and theory solving (dark shade).  NeuralSAT does not require parameter tuning and works out of the box, e.g., the rool runs on the wide-range of benchmarks in VNN-COMPs without any tuning.</td>
</tr>
<tr>
<td><strong>CORA</strong></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>7th</td>
<td>4th</td>
<td>GPU, MATLAB license</td>
<td>acasxu, cifar100, collins-rul-cnn, cora, dist-shift, nn4sys, safenlp, tinyimagenet, tlverifybench</td>
<td><a href="https://github.com/kollerlukas/cora-vnncomp2025">GitHub</a></td>
<td>CORA enables the formal verification of neural networks, both in open-loop as well as in closed-loop scenarios. Open-loop verification refers to the task where properties of the output set of a neural network are verified, e.g. correctly classified images given noisy input, as also considered at VNN-COMP. In closed-loop scenarios, the neural network is used as a controller of a dynamic system, e.g., controlling a car while keeping a safe distance over some time horizon.  This is realized using reachability analysis, mainly using polynomial zonotopes, allowing a non-convex enclosure of the output set of a neural network.</td>
</tr>
</tbody>
</table>
<h2 id="5-datasets-benchmarks">5. Datasets (Benchmarks)</h2>
<p><strong>We have selected the following benchmarks from VNN COMP, all focused on image verification. They are listed in descending order of complexity, as indicated by their parameter counts:</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Number of Instances</th>
<th>Application</th>
<th>Network Types</th>
<th>Parameters</th>
<th>Effective Input Dim</th>
<th>Track</th>
<th>Available Verification Tools</th>
<th>Links</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>cGAN</strong></td>
<td>20</td>
<td>Image Generation</td>
<td>Conv. + Vision Transformer</td>
<td>500k - 68M</td>
<td>5</td>
<td>regular</td>
<td>α-β-CROWN, nnenum, NeuralSAT, NNV, Marabou</td>
<td><a href="https://github.com/VNN-COMP/vnncomp2025_benchmarks/tree/main/benchmarks/cgan_2023">GitHub</a></td>
<td>The cGAN benchmark, proposed by Stony Brook University, addresses the verification of conditional generative adversarial networks (cGANs) for robust image classification. The network uses a 100×100×100 image space camera images based on a 1D distance condition and a 4D noise vector. The network under verification combines the generator and discriminator. Specifications check if the discriminator's predicted distance for the generated image aligns with the input condition, given constrained input ranges. Models vary in architecture (CNN and Vision Transformer) and image size (32x32, 64x64) to offer different difficulty levels.</td>
</tr>
<tr>
<td><strong>ViT</strong></td>
<td>200</td>
<td>Vision</td>
<td>Conv. + Residual + Softmax + BatchNorm</td>
<td>68k - 76k</td>
<td>3072</td>
<td>extended</td>
<td>α-β-CROWN, NeuralSAT, Marabou</td>
<td><a href="https://github.com/VNN-COMP/vnncomp2025_benchmarks/tree/main/benchmarks/vit_2023">GitHub</a></td>
<td>The ViT benchmark, proposed by the α-β-CROWN team, introduces verification tasks for Vision Transformers (ViTs) to address their complex architecture and diverse nonlinearities. It includes two modified, smaller ViT models (with 2-3 layers, 3 attention heads, and batch normalization replacing layer normalization) trained on the CIFAR-10 dataset. The model evaluates robustness verification under $\ell_\infty$ perturbations ($\epsilon=1/255$) on 100 carefully selected test instances per model, excluding easily verifiable or attackable examples. A 100-second timeout is set for each instance.</td>
</tr>
<tr>
<td><strong>cifar100</strong></td>
<td>200</td>
<td>Image Classification</td>
<td>FC + Conv. + Residual, ReLU + BatchNorm</td>
<td>2.5M - 3.8M</td>
<td>3072</td>
<td>regular</td>
<td>α-β-CROWN, NeuralSAT, NNV, CORA</td>
<td><a href="https://github.com/VNN-COMP/vnncomp2025_benchmarks/tree/main/benchmarks/cifar100_2024">GitHub</a></td>
<td>The CIFAR100 benchmark, proposed by the α-β-CROWN team, is a reused benchmark from VNN-COMP 2022 featuring two ResNet models of medium and large sizes trained on CIFAR-100. The models have 17-19 convolutional layers plus 10-10 convolutional layers. The model is trained on CIFAR-100 with different training methods. The model has 100-second timeout. Instances easily verified by vanilla CROWN were filtered out to increase difficulty, while about 18% of instances with adversarial examples were retained to help identify unsound verification results.</td>
</tr>
<tr>
<td><strong>CORA</strong></td>
<td>180</td>
<td>Image Classification</td>
<td>FC + ReLU</td>
<td>575k, 1.1M</td>
<td>784, 3072</td>
<td>regular</td>
<td>α-β-CROWN, nnenum, NeuralSAT, NNV, CORA</td>
<td><a href="https://github.com/VNN-COMP/vnncomp2025_benchmarks/tree/main/benchmarks/cora_2024">GitHub</a></td>
<td>The CORA benchmark, proposed by the CORA team, focuses on promoting fast neural network verification by using small timeout. It features a uniform ResNet architecture with three different training methods: standard, interval-bound propagation, and set-based training. The verification task is to confirm correct classification for all images within specified input sets.</td>
</tr>
</tbody>
</table>
<h2 id="6-verifiers-available-for-benchmarks">6. Verifiers available for Benchmarks</h2>
<p><strong>Since not every verifier is capable of being evaluated on all benchmarks, we have compiled the following table detailing their respective availabilities:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th><strong>cGAN</strong></th>
<th><strong>ViT</strong></th>
<th><strong>cifar100</strong></th>
<th><strong>CORA</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>α-β-CROWN</strong></td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td><strong>nnenum</strong></td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
<tr>
<td><strong>Marabou</strong></td>
<td>√</td>
<td>√</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td><strong>NNV</strong></td>
<td>√</td>
<td>×</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td><strong>NeuralSAT</strong></td>
<td>√</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td><strong>CORA</strong></td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
</tbody>
</table>
<h2 id="7-evaluation-metrics">7. Evaluation Metrics</h2>
<p><strong>For each benchmark, we provide the following evaluation matrix to assess the performance of verifiers:</strong></p>
<table>
<thead>
<tr>
<th>Evaluation Dimension</th>
<th>Calculation Method</th>
<th>Evaluation Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>Verified</td>
<td>Number of successfully verified instances</td>
<td>Measure the ability of verifiers to prove safety properties</td>
</tr>
<tr>
<td>Falsified</td>
<td>Number of successfully falsified instances</td>
<td>Evaluate the ability of verifiers to find counterexamples</td>
</tr>
<tr>
<td>Incorrect result</td>
<td>Number of instances with incorrect verification</td>
<td>Assess the situation of wrong answers from verifiers</td>
</tr>
<tr>
<td>Solved</td>
<td>Total number of solved instances</td>
<td>Core metric for comprehensive performance</td>
</tr>
<tr>
<td>Time</td>
<td>Total time to verify a benchmark</td>
<td>Measure the verification efficiency of verifiers</td>
</tr>
<tr>
<td>Rank</td>
<td>Rank verifiers based on total solved instances</td>
<td>Provide intuitive performance comparison</td>
</tr>
</tbody>
</table>

</body>
</html>
